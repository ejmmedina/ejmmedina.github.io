<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Elijah Justin Medina | Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="icon" href="../images/icon.svg">
		<link rel="stylesheet" href="../assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../index.html" class="logo"><strong>E. Medina</strong> | Home</a>
									<ul class="icons">
										<li><a href="https://www.linkedin.com/in/elijah-justin-medina/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://github.com/ejmmedina" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										<li><a href="mailto: elijahjustinmedina@gmail.com" class="icon solid fa-envelope"><span class="label">E-mail</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1 class="generic">Calculator, The Game: A Reinforcement Learning approach</h1>
										<h5 class="generic">By: Elijah Justin Medina</h5>
									</header>

									
									<span class="image main">
										<video width="75%" style="display:block; margin: 0 auto;" controls loop autoplay muted>
											<source src="../images/ml2-indiv-header.mp4" type="video/mp4">
										  Your browser does not support the video tag.
										  </video>
									</span>

									<h2>Highlights</h2>

									<div>
										<ul style="padding-left: 50px; padding-top: -10	px;">
											<li>Reinforcement Learning can be used to identify the steps to take in order to process an integer to another desired integer.</li>
											<li>The rules and rewards heavily affect the convergence of the learner and the optimal string of operations to perform on the agent.</li>
											<li>Given the current methodology (Q-Learning), this project cannot be scaled to Python objects that can take infinite number of states.</li>
										</ul>
									</div>

									<h2>Problem Statement</h2>

									<p>Data wrangling is a very tedious process, especially for those with minimal experience on programming. Most of the time, they would have an idea of what the form of their desired output is but at a loss on how to get there from the input data. For example, given an arbitrary Python object like a pandas Dataframe (read from a CSV), they would have to perform a string of operations to reach a desired outcome. If this desired outcome can be manually written, machine learning models can be used to map the input to the output. However, the process and steps to get there will remain a blackbox using traditional machine learning or even deep learning techniques. Our approach to this problem is the use of reinforcement learning wherein the agent is the Python object and the actions it takes are Python methods, changing its state until it becomes similar to the target output.</p>
									
									<img src="../images/ml2-indiv-header.PNG" width="60%" class="center" alt="" />
									<p>As an initial study, we start this project with <b>wrangling Python integers to a desired Python integer</b>. From this, we examine the feasibility of the project using Tabular Q-Learning and of further scaling it into other Python objects.</p>
									
									<h2>Introduction</h2>
									<p>One direct application of this project is finding the solution to Calculator: The Game, a game wherein there is a given input number, a target output number, and a set of actions that can be done on the number.</p>

									<h2>Methodology</h2>
									<p>In reinforcement learning, before anything else, the agent, actions, environment, observations, and corresponding rewards must be defined first.</p>

									<h3>Agent</h3>
									<p>The agent in this problem is the Python integer that is being processed. It can take different states limited to integers ranging from -500 to 500 (the limit was set to minimize the number of states our agent can take)</p>

									<h3>Actions</h3>
									<p>Hypothetically, the actions that the agent can take are all Python methods but for this project, there are four main actions: addition, subtraction, multiplication, or (integer) division with a number. For each main action, the number to add, subtract, multiply, or divide our agent with can vary. In this project, this number can take values from 1 to 9 for the addition/subtraction and prime numbers from 2 to 5 for the multiplication/division. The probabilities of taking each main actions is equal but the number to use has probabilities weighted by their inverse for the addition/subtraction and frequency for multiplication/division.</p>

									<h3>Observations</h3>
									<p>The observations of the agent here is constrained to where the agent's state currently is. For some of the rules implemented below, the agent can also observe if the certain action it took makes it nearer or farther to the target.</p>

									<h3>Rewards</h3>
									<p>The rewards for this learner is based on the following: reaching the goal number, reaching the boundary, getting closer to or farther from the target, or by each iteration. Different points are assigned to each scenario and these are varied as different rules.</p>

									<h3>Q-Learning</h3>
									<p>
										The algorithm used for this project is Tabular Q-Learning wherein a table of all possible states (rows) and all possible actions (columns) is created. The values are filled with the discounted future rewards, i.e. how "profitable" is it to take a certain action (column) given your current state (row). The update rules for for this Q-value is:						
									</p>
									<img src="../images/ml2-indiv-qlearning.png" class="center" width="75%">
									<p>In reinforcement learning, the action taken at each step can be one of two things: exploration or exploitation. Exploration is the completely random choice of which action to take while exploitation is taking the best next action based on the Q-Learning table. The chance of choosing exploration over exploitation ($\epsilon$) starts at 100% (pure exploration) and decays by 0.001% every 50 iterations. This is done until the 1M iteration at which point the final Q-Learning table is saved. Whenever it reaches the target output, the value reverts back to the initial value and resumes the iterations.</p>

									<p>If all states are explored, there will be 999 total possible states or rows (from -499 to 499) and 24 total possible actions or columns (9(add)+ 9(subtract) + 3(multiply) + 3(divide)) for the table.</p>
									<h2>Summary of Results / Main Learning points</h2>

									<h3>Convergence</h3>
									<p>Across different rule sets, most of the trials failed to converge properly. This is due to the fact that there are too many states that our agent can take and too many actions it can take. The reinforcement learner should be given more time to explore (i.e. more iterations and slower decay in probability of exploration) if this is the case. On the first attempt, the project was constrained to floats rather than integers, but this led to infinite possible states. So it becomes impossible for the learner to properly fill the Q-Learning table.</p>

									<p>The convergence of the learner is heavily dependent on the first few convergences. If the table is filled with the solution of minimal steps, the latter part of the iterations wherein the agent is mostly purely exploiting always converges with said minimal steps. Alternatively, some trials tend to be stuck to a periodic solution (e.g. it performs the same two operations every time), so an agent purely exploiting won't ever converge as it will only change between two values.</p>

									<h3>Resulting solution</h3>
									<p>The rules severely affect the behavior of the resulting solution. For instance, punishing the agent for getting farther from the target number causes the agent to take smaller steps as it is safer. However, this leads to higher number of steps.</p>

									<h3>Limitations and future work</h3>
									<p>Q-Learning cannot be used for Python object manipulation if the number of states it can take is infinite. Alternative reinforcement learning techniques or rules for rewards should be explored for this problem. As for scaling the project into different Python objects, it is feasible but the number of actions the agents can do must be clearly defined and the number of states is finite.</p>

									<h2>Acknowledgements</h2>
									<p>I would like to acknowledge Prof Chris and Prof Erika for sharing their knowledge to us which contributed immensely to the completion of this project. More importantly, Dr. Bunao for sharing his knowledge on reinforcement learning techniques, from which the codes here are based on. Lastly, my classmates for the exchange of ideas which helped alot for this project.</p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/calc-the-game/blob/master/calc-thegame-ml2.md" class="button">View code in GitHub</a></li>
									</ul>

									<p><i>If you have any questions regarding the analysis or wish to have a copy of the report, please contact me via <a href="mailto: elijahjustinmedina@gmail.com">e-mail</a> or <a href="https://www.linkedin.com/in/elijah-justin-medina/">LinkedIn</a></i></p>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
				
				<div id="sidebar">
					<div class="inner">

						<!-- Search -->
							<section id="search" class="alt">
								<form method="post" action="#">
									<input type="text" name="query" id="query" placeholder="Search" />
								</form>
							</section>

						<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Homepage</a></li>
									
									<li>
										<span class="opener">Machine Learning</span>
										<ul>
											<li><a href="../projects/ML2-group-project.html">Rice Disease Image Classification</a></li>
											<li><a href="../projects/ML2-indiv-project.html">Calculator, The Game</a></li>
											<li><a href="../projects/ML-indiv-project.html">What's that Pokemon?</a></li>
										</ul>
									</li>
									<li>
										<span class="opener">Big Data and Cloud Computing</span>
										<ul>
											<li><a href="../projects/DMW-project.html">Upgrading Amenities</a></li>
										</ul>
									</li>
									<li>
										<span class="opener">Data Mining and Wrangling</span>
										<ul>
											<li><a href="../projects/DMW-project.html">Unstack the Flow</a></li>
										</ul>
									</li>
									<li>
										<span class="opener">Web Application (Deployment)</span>
										<ul>
											<li><a href="../projects/DAW-indiv-project.html">Basic Image Pre-processing Techniques</a></li>
											<li><a href="https://github.com/ejmmedina/rice-disease-classification-webapp">Rice Disease Image Classification</a></li>
										</ul>
									</li>
									<li>
										<span class="opener">Network Science</span>
										<ul>
											<li><a href="../projects/NS-project.html">Analysis of Philippine HCPN Network</a></li>
										</ul>
									</li>
								</ul>
							</nav>

						<!-- Section -->
						<section>
							<header class="major">
								<h2>Mini-Projects</h2>
							</header>
							<div class="mini-posts">
								<article>
									<h4>Predicting Speed of News Dissemination using Machine Learning Algorithm</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>news, machine-learning, big-data, cloud-computing</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/news-dissemination-analysis" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>Who would you listen to?: A Frequent Itemset Mining and Recommender System approach on the Million Song Dataset</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>million-song-dataset, frequent-itemset-mining, recommender-system</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/artist-genre-fim-rs/blob/master/artist-genre-fim-rs-bdcc.md" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>Comparison of Different Algorithms in Predicting Bike Sharing Hourly Count</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>machine-learning, neural-network, bike-sharing</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/bike-sharing-nn/blob/master/bike-sharing.md" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>Party-Lists of the 2019 Elections: The Winners and The Real Score</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>election-data, geospatial-visualization</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/party-list-representation/blob/master/party-list-representation-dmw.md" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>The Winners' Circle: Ranking Chess' Grandest Of All Time</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>chess-game, network-analysis</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/chess-ranking/blob/master/chess-ranking-dmw.md" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>Cluster Analysis of Reddit Posts</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>reddit, clustering</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/reddit-clustering/blob/master/reddit-clustering-dmw.md" class="button">View code</a></li>
									</ul>
								</article>
								<article>
									<h4>An alternative to Nomis solutions on e-Cars case through Logistic Regression with Lasso regularization</h4>
									<p style="margin-bottom: 10px;"><b>Tags:</b> <i>loan-prediction-analysis, machine-learning, revenue-optimization</i></p>
									<ul class="actions">
										<li><a href="https://github.com/ejmmedina/car-loan-optimization/blob/master/pricing-optimization-ml.md" class="button">View code</a></li>
									</ul>
								</article>
							</div>
						</section>


						<!-- Section -->
							<section>
								<header class="major">
									<h2>Contact Me</h2>
								</header>
								<p>If you have any questions, please contact me through the following:</p>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="#">elijahjustinmedina@gmail.com</a></li>
									<li class="icon solid fa-phone">(+63)9274784006</li>
									<li class="icon solid fa-home">1205 Silk Residences Tower 1<br />
									Manila, Philippines</li>
								</ul>
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">Template: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

					</div>
				</div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>